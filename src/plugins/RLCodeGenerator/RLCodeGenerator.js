/*globals define*/
/*eslint-env node, browser*/

/**
 * Generated by PluginGenerator 2.20.5 from webgme.
 */

define([
    'plugin/PluginConfig',
    'text!./metadata.json',
    'plugin/PluginBase'
], function (
    PluginConfig,
    pluginMetadata,
    PluginBase) {
    'use strict';

    pluginMetadata = JSON.parse(pluginMetadata);

    /**
     * Initializes a new instance of RLCodeGenerator.
     * @class
     * @augments {PluginBase}
     * @classdesc This class represents the plugin RLCodeGenerator.
     * @constructor
     */
    function RLCodeGenerator() {
        // Call base class' constructor.
        PluginBase.call(this);
        this.pluginMetadata = pluginMetadata;
    }

    /**
     * Metadata associated with the plugin. Contains id, name, version, description, icon, configStructure etc.
     * This is also available at the instance at this.pluginMetadata.
     * @type {object}
     */
    RLCodeGenerator.metadata = pluginMetadata;

    // Prototypical inheritance from PluginBase.
    RLCodeGenerator.prototype = Object.create(PluginBase.prototype);
    RLCodeGenerator.prototype.constructor = RLCodeGenerator;

    /**
     * Main function for the plugin to execute.
     */
    RLCodeGenerator.prototype.main = function (callback) {
        const self = this;
        const core = self.core;
        const activeNode = self.activeNode;

        // 1. Create an Artifact to hold the generated files (this allows downloading them as a zip)
        const artifact = self.blobClient.createArtifact('RL_Training_Scripts');

        // 2. Load all children of the active node (The Folder)
        core.loadChildren(activeNode)
            .then(function (children) {
                // Filter specifically for "Training_Run" nodes
                const runNodes = children.filter(child => {
                    const metaType = core.getMetaType(child);
                    return metaType && core.getAttribute(metaType, 'name') === 'Training_Run';
                });

                if (runNodes.length === 0) {
                    self.logger.warn('No Training_Run nodes found in the active folder.');
                }

                // Process all runs concurrently
                const promises = runNodes.map(runNode => self.processTrainingRun(runNode, artifact));
                return Promise.all(promises);
            })
            .then(function () {
                // 3. Save the artifact and finish
                return artifact.save();
            })
            .then(function (artifactHash) {
                self.result.addArtifact(artifactHash);
                self.result.setSuccess(true);
                self.logger.info('Artifact saved with hash: ' + artifactHash);
                callback(null, self.result);
            })
            .catch(function (err) {
                self.logger.error(err.stack);
                callback(err, self.result);
            });
    };

    /**
     * Helper to process a single Training Run node.
     * resolving pointers to Agent and Environment, and finding the Architecture.
     */
    RLCodeGenerator.prototype.processTrainingRun = function (runNode, artifact) {
        const self = this;
        const core = self.core;
        const runName = core.getAttribute(runNode, 'name');
        
        self.logger.info(`Processing Training Run: ${runName}`);

        // Extract Training Run Attributes
        const runParams = {
            batch_size: core.getAttribute(runNode, 'batch_size'),
            timesteps: core.getAttribute(runNode, 'timesteps')
        };

        // Prepare variables to hold the loaded nodes
        let agentNode = null;
        let envNode = null;
        let architectureNode = null;

        // Get Pointer Paths
        const agentPath = core.getPointerPath(runNode, 'Agent');
        const envPath = core.getPointerPath(runNode, 'Environment');

        if (!agentPath || !envPath) {
            self.logger.error(`Run ${runName} is missing a pointer to Agent or Environment.`);
            return Promise.resolve(); // Skip this run but don't crash
        }

        // Load Agent and Environment
        return core.loadByPath(self.rootNode, agentPath)
            .then(function (node) {
                agentNode = node;
                return core.loadByPath(self.rootNode, envPath);
            })
            .then(function (node) {
                envNode = node;
                // Now load Agent's children to find the Architecture
                return core.loadChildren(agentNode);
            })
            .then(function (agentChildren) {
                // Find the Architecture node inside the Agent
                architectureNode = agentChildren.find(child => {
                    const meta = core.getMetaType(child);
                    return meta && core.getAttribute(meta, 'name') === 'Architecture';
                });

                // Extract all remaining parameters
                const envId = core.getAttribute(envNode, 'env_id');
                
                const agentParams = {
                    policy: core.getAttribute(agentNode, 'policy'),
                    learning_rate: core.getAttribute(agentNode, 'learning_rate'),
                    discount_factor: core.getAttribute(agentNode, 'discount_factor'),
                    initial_epsilon: core.getAttribute(agentNode, 'initial_epsilon'),
                    final_epsilon: core.getAttribute(agentNode, 'final_epsilon'),
                    epsilon_decay: core.getAttribute(agentNode, 'epsilon_decay')
                };

                const archParams = {
                    // Default values if architecture is missing
                    layer_size: 64, 
                    activation: 'ReLU' 
                };

                if (architectureNode) {
                    archParams.layer_size = core.getAttribute(architectureNode, 'layer_size');
                    // Note: accessing 'activation_funtion' (sic) based on your meta-model
                    archParams.activation = core.getAttribute(architectureNode, 'activation_funtion');
                } else {
                    self.logger.warn(`Agent for run ${runName} has no Architecture defined. Using defaults.`);
                }

                // Generate the Python Code
                const codeContent = self.generatePythonCode(runName, envId, runParams, agentParams, archParams);

                // Add to artifact
                return artifact.addFile(`${runName}.py`, codeContent);
            });
    };

    /**
     * Generates the Python script string.
     */
    RLCodeGenerator.prototype.generatePythonCode = function (runName, envId, runP, agentP, archP) {
        
        // Map activation string to PyTorch syntax
        let activationCode = 'nn.ReLU()';
        if (archP.activation === 'Tanh') activationCode = 'nn.Tanh()';
        if (archP.activation === 'Sigmoid') activationCode = 'nn.Sigmoid()';

        // Use Template Literals (backticks) for multi-line string
        return `import gymnasium as gym
import math
import random
import torch
import torch.nn as nn
import torch.optim as optim

# ==========================================
# GENERATED BY WEBGME
# TRAINING RUN: ${runName}
# ==========================================

# --- Configuration ---
ENV_ID = "${envId}"
BATCH_SIZE = ${runP.batch_size}
TOTAL_TIMESTEPS = ${runP.timesteps}

GAMMA = ${agentP.discount_factor}
EPS_START = ${agentP.initial_epsilon}
EPS_END = ${agentP.final_epsilon}
EPS_DECAY = ${agentP.epsilon_decay}
LR = ${agentP.learning_rate}
LAYER_SIZE = ${archP.layer_size}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DQN(nn.Module):
    def __init__(self, n_observations, n_actions):
        super(DQN, self).__init__()
        self.layer1 = nn.Linear(n_observations, LAYER_SIZE)
        self.layer2 = nn.Linear(LAYER_SIZE, LAYER_SIZE)
        self.layer3 = nn.Linear(LAYER_SIZE, n_actions)
        self.activation = ${activationCode}

    def forward(self, x):
        x = self.activation(self.layer1(x))
        x = self.activation(self.layer2(x))
        return self.layer3(x)

def train():
    print(f"Starting training for ${runName} on {ENV_ID}")
    env = gym.make(ENV_ID)
    
    n_actions = env.action_space.n
    state, _ = env.reset()
    n_observations = len(state)

    policy_net = DQN(n_observations, n_actions).to(device)
    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
    
    steps_done = 0
    state, _ = env.reset()
    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

    # Simplified training loop
    for t in range(TOTAL_TIMESTEPS):
        sample = random.random()
        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\
            math.exp(-1. * steps_done / EPS_DECAY)
        steps_done += 1
        
        if sample > eps_threshold:
            with torch.no_grad():
                action = policy_net(state).max(1)[1].view(1, 1)
        else:
            action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)

        observation, reward, terminated, truncated, _ = env.step(action.item())
        done = terminated or truncated

        if terminated:
            next_state = None
        else:
            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)
            
        state = next_state
        if done:
            state, _ = env.reset()
            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)

    print("Training Complete")

if __name__ == '__main__':
    train()
`;
    };

    return RLCodeGenerator;
});